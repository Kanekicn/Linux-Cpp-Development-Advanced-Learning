# [强化学习_1] 强化学习基本概念及马尔科夫决策过程

[TOC]

## 1 Overview

​      强化学习作为机器学习的一个领域，主要研究智能体如何在环境(environment)中做出动作(action)，以最大化环境中获得的奖励(reward)。强化学习主要研究环境中对象的行为以及如何最优化其行为，而在游戏领域中，正是强化学习所研究的内容。2016年，DeepMind的AlphGo取得围棋冠军；OpenAI的open five在DOTA游戏中打败职业选手。

​    本篇教程记录强化学习的基础，包括马尔科夫决策过程，Q-learning，DQN，Bellman Equations，policy gradient.

## 2 MDP

​    一个马尔科夫过程：agent与environment交互，对于environment的representation of state，agent做出action，environment将转换为new state，agent获得之前序列action的reward。

MDP 的组成：

- Agent
- Environment
- State
- Action
- Reward

   

## 3 期望回报

Episodic vs. continuing tasks

$G_{t}=R_{t+1}+R_{t+2}+R_{t+3}+\cdots+R_{T}​$



$\begin{aligned} G_{t} &=R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} R_{t+3}+\cdots \\ &=R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\gamma^{2} R_{t+3}+\cdots\right) \\ &=R_{t+1}+\gamma G_{t+1} \end{aligned}$



## 4 Policy 和 Value Funtion

| Question                                                     | Addressed by    |
| ------------------------------------------------------------ | --------------- |
| *How probable* is it for an agent to select any action from a given state? | Policies        |
| *How good* is any given action or any given state for an agent? | Value functions |

### Policy 

for each state $s \in S​$ ,$\pi​$ is a probability distribution over $a \in A(s)​$.

### 状态价值函数（State-value function）

​    从初始状态$\mathcal{s_0}​$出发，在策略$\pi​$下，在$\mathcal{t}​$时刻状态$\mathcal{s}​$的值函数：

​                   $\begin{aligned} v_{\pi}(s) &=E_{\pi}\left[G_{t} | S_{t}=s\right] \\ &=E_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} | S_{t}=s\right] \end{aligned}​$

### 动作价值函数（Action-value function)

​    从初始状态$\mathcal{s_0}​$出发，在策略$\pi​$下，在$\mathcal{t}​$时刻状态$\mathcal{a}​$的值函数：

​                      $\begin{aligned} q_{\pi}(s, a) &=E_{\pi}\left[G_{t} | S_{t}=s, A_{t}=a\right] \\ &=E_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} | S_{t}=s, A_{t}=a\right] \end{aligned}​$          

​    即该动作在状态s下的q值，称为Q-value。'Q' :the *quality* of taking a given action in a given state

### Difference of State-value function and Action-value function?

$V(s)=\sum_{a} \pi(a | s) Q(s, a)​$

状态价值函数 = 状态$\mathcal{s}$在策略$\pi$下，所有动作概率 * 该动作的value 之和

<div align=center>
<img src='E:\学习\研究生\研0\0. 强化学习\images\relationOfVAndQ.png' width='800'>
</div>

## 5 Bellman optimality equation

### 最优策略

$\pi \geq \pi^{\prime}$ if and only if $v_{\pi}(s) \geq v_{\pi^{\prime}}(s)$ for all $s \in S​$

最优状态价值函数：

$v_{*}(s)=\max _{\pi} v_{\pi}(s)$

最优动作价值函数：

$q_{*}(s, a)=\max _{\pi} q_{\pi}(s, a)​$

$q_{*}​$的Bellman最优化方程：

$q_{*}(s, a)=E\left[R_{t+1}+\gamma \max _{a^{\prime}} q_{*}\left(s^{\prime}, a^{\prime}\right)\right]​$







## 参考

1. [状态价值函数与动作价值函数的不同](https://www.quora.com/In-reinforcement-learning-what-is-the-difference-between-a-state-value-function-V-s-and-a-state-action-value-function-Q-s-a)
2. [强化学习blog](http://deeplizard.com/learn/video/rP4oEpQbDm4)
3. [RL_PPT](https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture26-ri.pdf)